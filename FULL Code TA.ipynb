{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if u haven't install the google-play scrapper library u can install it by this code:\n",
    "!pip install google-play-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43888da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_play_scraper import Sort, reviews_all, reviews, app\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79739808",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, continuation_token = reviews(\n",
    "    'org.detikcom.rss',\n",
    "    lang='id',                \n",
    "    country='id',             \n",
    "    sort=Sort.MOST_RELEVANT,  \n",
    "    count=10000,               \n",
    "    filter_score_with=None    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfdf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.array(result),columns=['review'])\n",
    "\n",
    "data = data.join(pd.DataFrame(data.pop('review').tolist()))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrappeddata1 = data[['content','score','at']]\n",
    "data = scrappeddata1.sort_values(by='at', ascending=False) #Sort by Newest, change to True if you want to sort by Oldest.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d80f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "positive = sample_reviews[sample_reviews['score'] > 3]\n",
    "neutral = sample_reviews[sample_reviews['score'] == 3]\n",
    "negative = sample_reviews[sample_reviews['score'] < 3]\n",
    "scores = ('Positive','Neutral', 'Negative')\n",
    "y_pos = np.arange(len(scores))\n",
    "numscores = [len(positive), len(neutral),len(negative)]\n",
    " \n",
    "plt.bar(scores, numscores, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, scores)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of Reviews with its Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(data['at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(data['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356dc4c2",
   "metadata": {},
   "source": [
    "## Pelabelan menggunakan metode VADER sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment score for each review\n",
    "vader_sentiment = SentimentIntensityAnalyzer()\n",
    "data['scores_VADER'] = data['content'].apply(lambda s: vader_sentiment.polarity_scores(s)['compound'])\n",
    "# Predict sentiment label for each review\n",
    "data['pred_VADER'] = data['scores_VADER'].apply(lambda x: 1 if x >=0 else 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a238b6",
   "metadata": {},
   "outputs": [],
   "source": [
    " Check the counts of labels\n",
    "data['pred_VADER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random split train and test data\n",
    "index = data.index\n",
    "data['random_number'] = np.random.randn(len(index))\n",
    "train = data[data['random_number'] <= 0.8]\n",
    "test = data[data['random_number'] > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c76a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "train_matrix = vectorizer.fit_transform(train['content'])\n",
    "test_matrix = vectorizer.transform(test['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fdcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_matrix\n",
    "X_test = test_matrix\n",
    "y_train = train['pred_VADER']\n",
    "y_test = test['pred_VADER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accuracy, precision, recall:\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59731c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "f, ax = plt.subplots(figsize=(8,5))\n",
    "sns.heatmap(confusion_matrix(predictions, y_test), annot=True, fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_head\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1af35",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#case folding\n",
    "def clean_lower(lwr):\n",
    "    lwr = lwr.lower() # lowercase text\n",
    "    return lwr\n",
    "a['lwr'] = a['content'].apply(clean_lower)\n",
    "\n",
    "#Remove Puncutuation\n",
    "clean_spcl = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "clean_symbol = re.compile('[^0-9a-z]')\n",
    "def clean_punct(text):\n",
    "    text = clean_spcl.sub('', text)\n",
    "    text = clean_symbol.sub(' ', text)\n",
    "    return text  \n",
    "a['clean_punct'] = a['lwr'].apply(clean_punct)\n",
    "\n",
    "#remove whitespace\n",
    "def _normalize_whitespace(text):\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
    "    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
    "    return corrected.strip(\" \")\n",
    "a['clean_double_ws'] = a['clean_punct'].apply(_normalize_whitespace)\n",
    "\n",
    "#porterstemmer\n",
    "!pip install nltk\n",
    "import nltk \n",
    "wn= nltk.WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    text = ' '.join(wn.lemmatize(word) for word in text.split() if word in text)\n",
    "    return text\n",
    "# Buat kolom tambahan untuk data description yang telah dilemmatization   \n",
    "a['desc_clean_porterstem'] = a['clean_double_ws'].apply(lemmatization)\n",
    "\n",
    "#stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "#clean stopwords\n",
    "stopword = set(stopwords.words('english'))\n",
    "def clean_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split() if word not in stopword) # hapus stopword dari kolom deskripsi\n",
    "    return text\n",
    "# Buat kolom tambahan untuk data description yang telah distopwordsremoval   \n",
    "a['clean_sw'] = a['desc_clean_porterstem'].apply(clean_stopwords)\n",
    "add = pd.DataFrame(a['clean_sw'])\n",
    "a['add_swr']= add.replace(to_replace =['donk','sll'],  \n",
    "                            value =\"\", regex= True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8329a3",
   "metadata": {},
   "source": [
    "## Pembobotan tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337060ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perhitungan TF\n",
    "def calc_TF(document):\n",
    "    # Counts the number of times the word appears in review\n",
    "    TF_dict = {}\n",
    "    for term in document:\n",
    "        if term in TF_dict:\n",
    "            TF_dict[term] += 1\n",
    "        else:\n",
    "            TF_dict[term] = 1\n",
    "    # Computes tf for each word\n",
    "    for term in TF_dict:\n",
    "        TF_dict[term] = TF_dict[term] / len(document)\n",
    "    return TF_dict\n",
    "\n",
    "sample_reviews[\"TF_dict\"] = sample_reviews['text_stem'].apply(calc_TF)\n",
    "\n",
    "sample_reviews[\"TF_dict\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ee03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TF result (Hasil perhitungan TF)\n",
    "index = 15\n",
    "\n",
    "print('%20s' % \"term\", \"\\t\", \"TF\\n\")\n",
    "for key in sample_reviews[\"TF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", sample_reviews[\"TF_dict\"][index][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43286d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_DF(tfDict):\n",
    "    count_DF = {}\n",
    "    # Run through each document's tf dictionary and increment countDict's (term, doc) pair\n",
    "    for document in tfDict:\n",
    "        for term in document:\n",
    "            if term in count_DF:\n",
    "                count_DF[term] += 1\n",
    "            else:\n",
    "                count_DF[term] = 1\n",
    "    return count_DF\n",
    "\n",
    "DF = calc_DF(sample_reviews[\"TF_dict\"])\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b803072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_document = len(sample_reviews)\n",
    "\n",
    "def calc_IDF(__n_document, __DF):\n",
    "    IDF_Dict = {}\n",
    "    for term in __DF:\n",
    "        IDF_Dict[term] = np.log(__n_document / (__DF[term] + 1))\n",
    "    return IDF_Dict\n",
    "  \n",
    "#Stores the idf dictionary\n",
    "IDF = calc_IDF(n_document, DF)\n",
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a08796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc TF-IDF\n",
    "def calc_TF_IDF(TF):\n",
    "    TF_IDF_Dict = {}\n",
    "    #For each word in the review, we multiply its tf and its idf.\n",
    "    for key in TF:\n",
    "        TF_IDF_Dict[key] = TF[key] * IDF[key]\n",
    "    return TF_IDF_Dict\n",
    "\n",
    "#Stores the TF-IDF Series\n",
    "sample_reviews[\"TF-IDF_dict\"] = sample_reviews[\"TF_dict\"].apply(calc_TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134fffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TF-IDF result\n",
    "index = 15\n",
    "\n",
    "print('%20s' % \"term\", \"\\t\", '%10s' % \"TF\", \"\\t\", '%20s' % \"TF-IDF\\n\")\n",
    "for key in sample_reviews[\"TF-IDF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", sample_reviews[\"TF_dict\"][index][key] ,\"\\t\" , sample_reviews[\"TF-IDF_dict\"][index][key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46537e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort descending by value for DF dictionary \n",
    "sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)[:50]\n",
    "\n",
    "# Create a list of unique words from sorted dictionay `sorted_DF`\n",
    "unique_term = [item[0] for item in sorted_DF]\n",
    "\n",
    "def calc_TF_IDF_Vec(__TF_IDF_Dict):\n",
    "    TF_IDF_vector = [0.0] * len(unique_term)\n",
    "\n",
    "    # For each unique word, if it is in the review, store its TF-IDF value.\n",
    "    for i, term in enumerate(unique_term):\n",
    "        if term in __TF_IDF_Dict:\n",
    "            TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "    return TF_IDF_vector\n",
    "\n",
    "sample_reviews[\"TF_IDF_Vec\"] = sample_reviews[\"TF-IDF_dict\"].apply(calc_TF_IDF_Vec)\n",
    "\n",
    "print(\"print first row matrix TF_IDF_Vec Series\\n\")\n",
    "print(sample_reviews[\"TF_IDF_Vec\"][0])\n",
    "\n",
    "print(\"\\nmatrix size : \", len(sample_reviews[\"TF_IDF_Vec\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb644034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Series to List\n",
    "TF_IDF_Vec_List = np.array(sample_reviews[\"TF_IDF_Vec\"].to_list())\n",
    "\n",
    "# Sum element vector in axis=0 \n",
    "sums = TF_IDF_Vec_List.sum(axis=0)\n",
    "\n",
    "data = []\n",
    "\n",
    "for col, term in enumerate(unique_term):\n",
    "    data.append((term, sums[col]))\n",
    "    \n",
    "ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "ranking.sort_values('rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59878b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranking.head(11).plot.bar(x='term', y='rank', rot=0,figsize=(40,20));\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.bar(x=ranking['term'],\n",
    " \n",
    "        height=ranking['rank'],\n",
    "       color='midnightblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.title('TF_IDF_Vec_List')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a9d75",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75651778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "!pip install plotly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_colors = [\"#17C37B\", \"#F92969\", \"#FACA0C\"]\n",
    "list_titles = [\"Positive Sentiment\", \"Negative Sentiment\", \"Neutral Sentiment\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 20), sharey=True, dpi=160)\n",
    "\n",
    "for i, ax in enumerate (axes.flatten()):\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        max_words=50,\n",
    "        min_font_size=10,\n",
    "        background_color=\"white\",\n",
    "        colormap=\"tab10\",\n",
    "        color_func=lambda *args, **kwargs: list_colors[i],\n",
    "        stopwords=None,\n",
    "        prefer_horizontal=1.0\n",
    "    )\n",
    "    fig.add_subplot(ax)\n",
    "    wc.generate(sample_reviews['text_number'][i])\n",
    "    plt.gca().imshow(wc)\n",
    "    plt.gca().set_title(list_titles[i], fontdict=dict(size=16))\n",
    "    plt.gca().axis(\"off\")\n",
    "    \n",
    "plt.axis(\"off\")\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d24ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the df to one candidate, and create a list of responses from them\n",
    "text = sample_reviews['content'].tolist() \n",
    "\n",
    "# join the list and lowercase all the words\n",
    "text = ' '.join(text).lower()\n",
    "\n",
    "# change the value to black\n",
    "def black_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "    return(\"hsl(0,100%, 1%)\")\n",
    "#create the wordcloud object\n",
    "wordcloud = WordCloud(stopwords = STOPWORDS,background_color=\"white\", width=3000, height=2000, max_words=500,\n",
    "                      collocations=True).generate(text)\n",
    "# set the word color to black\n",
    "wordcloud.recolor(color_func = black_color_func)\n",
    "#plot the wordcloud object\n",
    "plt.imshow(wordcloud, interpolation='bilInear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca510d",
   "metadata": {},
   "source": [
    "## Classification with MaxEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "def word_feats(words):\n",
    "\treturn dict([(word, True) for word in words])\n",
    "\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "negcutoff = int(len(negfeats)*3/4)\n",
    "poscutoff = int(len(posfeats)*3/4)\n",
    "\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "print('train on %d instances, test on %d instances - Maximum Entropy' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "classifier = MaxentClassifier.train(trainfeats, 'GIS', trace=0, encoding=None, labels=None, gaussian_prior_sigma=0, max_iter = 1)\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(testfeats):\n",
    "\trefsets[label].add(i)\n",
    "\tobserved = classifier.classify(feats)\n",
    "\ttestsets[observed].add(i)\n",
    " \n",
    "accuracy = nltk.classify.util.accuracy(classifier, testfeats)\n",
    "pos_precision = precision(refsets['pos'], testsets['pos'])\n",
    "pos_recall = recall(refsets['pos'], testsets['pos'])\n",
    "pos_fmeasure = f_measure(refsets['pos'], testsets['pos'])\n",
    "neg_precision = precision(refsets['neg'], testsets['neg'])\n",
    "neg_recall = recall(refsets['neg'], testsets['neg'])\n",
    "neg_fmeasure =  f_measure(refsets['neg'], testsets['neg'])\n",
    "\t\t\n",
    "print('')\n",
    "print ('---------------------------------------')\n",
    "print ('          Maximum Entropy              ')\n",
    "print ('---------------------------------------')\n",
    "print ('accuracy:', accuracy)\n",
    "print ('precision', (pos_precision + neg_precision) / 2)\n",
    "print ('recall', (pos_recall + neg_recall) / 2)\n",
    "print ('f-measure', (pos_fmeasure + neg_fmeasure) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deced24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
